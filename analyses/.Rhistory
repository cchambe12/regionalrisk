#
# #Actually want to be solving for and minimizing J (post-quadratic) and minimizing, THEN subtracting R
#
# J1 <- function(Ci, input.df) {
#    J <- ((input.df$Vcmax * (Ci - input.df$comp)) / (Ci + input.df$X))
#    return(J)
# }
#
# J2 <- function(Ci, input.df) {
#    J <- (alpha * input.df$PAR * ((Ci - input.df$comp) / (Ci + 2 * input.df$comp)))
#    return(J)
# }
#
### stomata closed ###
if(stomata == 'closed') {
### CO2 limited case, coefficients for polyroot function
aa <- (b * input.df$X * input.df$Ca/1.6) + (input.df$Vcmax * input.df$comp) + (input.df$Rd * input.df$X)
bb <- (input.df$Rd) + (b * input.df$Ca / 1.6) - (b * input.df$X / 1.6) - (input.df$Vcmax)
cc <- (-b / 1.6)
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
#coerce into non-imaginary components
roots.num <- Re(roots)
#extract the non-negative value
Ci.extract.A1 <- apply(roots.num, 2, max)
# calculate A
AA1 <- A1(Ci = Ci.extract.A1, input.df = input.df)
### Light limited case, coefficients for polyroot function
aa <- ((b * 2 * input.df$comp * input.df$Ca / 1.6) + (input.df$Rd * 2 * input.df$comp) + (input.df$Y * input.df$comp))
bb <- (input.df$Rd + (b * input.df$Ca / 1.6) - (b * 2 * input.df$comp / 1.6) - input.df$Y)
cc <- (-b / 1.6)
# define polynomial roots for each data point
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
#coerce into non-imaginary components
roots.num <- Re(roots)
# extract the non-negative value
Ci.extract.A2 <- apply(roots.num, 2, max)
# calculate A2
AA2 <- A2(Ci = Ci.extract.A2, input.df = input.df)  # only works if PAR has values 6 orders of magnitude higher
### Build output data frame
# pick minimum for each time point:
A.df <- data.frame (AA1 = AA1, AA2 = AA2, Ci.A1 = Ci.extract.A1, Ci.A2 = Ci.extract.A2)
A.df$A.min <- apply(A.df[,1:2], 1, min)
A.df$min.eq <- apply(A.df[,1:2], 1, which.min)
## Solve for gsw ##
# stomata closed, gsw = b
A.df$gsw <- rep(b, dim(A.df)[1])
}
#### Stomata Open ###
if(stomata == 'open') {
############ vvvvv THIS IS WRONG!!!!!!! vvvvv ************
### CO2 limited coefficients
aa <- ((b * input.df$X * input.df$Ca * input.df$FF) + (1.6 * input.df$FF * input.df$Vcmax * input.df$comp) - (1.6 * input.df$FF * input.df$Rd * input.df$X) -
(M * input.df$Vcmax * input.df$comp * input.df$Ca) + (M * input.df$Rd * input.df$X * input.df$Ca))
bb <- ((-input.df$FF * b * input.df$Ca) - (b * input.df$X) - (1.6 * input.df$FF * input.df$Vcmax) + (1.6 * input.df$FF * input.df$Rd) + (M * input.df$Vcmax * input.df$Ca) +
(M * input.df$Vcmax * input.df$comp) - (input.df$Rd * M * input.df$Ca) + (M * input.df$Rd * input.df$X))
cc <- ((-b * input.df$FF) + (M * input.df$Vcmax) - (M * input.df$Rd))
############ ^^^^^ THIS IS WRONG!!!!!!! ^^^^^ ************
# define polynomial roots for each data point
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
#coerce into non-imaginary components
roots.num <- Re(roots)
#extract the non-negative value
Ci.extract.A1 <- apply(roots.num, 2, max)
#calculate A1
AA1 <- A1(Ci = Ci.extract.A1, input.df = input.df)
### Light-limited coefficients
aa <- ((b * input.df$FF * input.df$Ca * 2 * input.df$comp) - (1.6 * input.df$FF * input.df$Y * input.df$comp) + (1.6 * input.df$FF * input.df$Rd * 2 * input.df$comp) -
(M * input.df$Ca * input.df$Y * input.df$comp) + (M * input.df$Rd * 2 * input.df$comp * input.df$Ca))
bb <- ((b * input.df$FF * input.df$Ca) - (b * input.df$FF * 2 * input.df$comp) - (1.6 * input.df$FF * input.df$Y) + (1.6 * input.df$FF * input.df$Rd) + (M * input.df$Y * input.df$Ca) +
(M * input.df$Y * input.df$comp) - (M * input.df$Rd * input.df$Ca) - (M * input.df$Rd * 2 * input.df$comp))
cc <- ((-b * input.df$FF) - (M * input.df$Y) + (M * input.df$Rd))
# define polynomial roots for each data point
z <- data.frame(aa = aa, bb = bb, cc = cc)  # where aa + bb*c1 + cc*c1^2
#solve the polynomial
roots <- apply(z, 1, polyroot)
if(round(Im(roots[1]), 10) != 0) {
stop("quadratic roots are imaginary")
}
# coerce into non-imaginary components
roots.num <- Re(roots)
# extract the non-negative value
Ci.extract.A2 <- apply(roots.num, 2, max)
# calculate A2
AA2 <- A2(Ci = Ci.extract.A2, input.df = input.df)  # only works if PAR has values 6 orders of magnitude higher
### build output data frame
# pick minimum for each time point:
A.df <- data.frame (AA1 = AA1, AA2 = AA2, Ci.A1 = Ci.extract.A1, Ci.A2 = Ci.extract.A2)
A.df$A.min <- apply(A.df[,1:2], 1, min)
A.df$min.eq <- apply(A.df[,1:2], 1, which.min)
### solve for gsw ###
gsw.solve <- ((M * A.df$A.min)/((input.df$Ca - input.df$comp) * (1 + ((input.df$el - input.df$ea)/Do)))) + b
A.df$gsw <- gsw.solve
}
return(A.df)
}
##### Applying the farquhar_solver function #####
farquhar_solver(input.df = dummy, stomata = 'closed')
farquhar_solver(input.df = dummy, stomata = 'open')
#### Check against plantecophs package... ####
dummy$el <- el(Tv = dummy$Tv)
dummy$ea <- ea(relHum = dummy$relHum, Tv = dummy$Tv)
dummy$VPD<- dummy$el-dummy$ea
dummy$Tair<-dummy$Tv-273.15
FARAO(Ca=dummy$Ca, VPD=dummy$VPD, Tair=dummy$Tair)
#### Now look at real data to compare ####
dat <- read.csv('~/Documents/git/scaling_farquhar/Aggregated_Climate_Data.csv', header=TRUE)
dat$Tv <- dat$Air_Temp_K
dat$Tv<-ave(dat$Tv, dat$month, dat$year, dat$day, dat$hour)
dat$relHum <- dat$Relative_Humidity_Percent
dat$relHum<-ave(dat$relHum, dat$month, dat$year, dat$day, dat$hour)
dat$Ca <- as.numeric(dat$Atmospheric_CO2)
dat$Ca <- ave(dat$Ca, dat$month, dat$year, dat$day, dat$hour)
dat$comp <- comp(Tv = dat$Tv)
dat$comp<-ave(dat$comp, dat$month, dat$year, dat$day, dat$hour)
dat$Vcmax <- Vcmax(Vmo = 35, Tv = dat$Tv, Tvlo=277.85)
dat$Vcmax<-ave(dat$Vcmax, dat$month, dat$year, dat$day, dat$hour)
dat$el <- el(Tv = dat$Tv)
dat$el<-ave(dat$el, dat$month, dat$year, dat$day, dat$hour)
dat$ea <- ea(relHum = dat$relHum, Tv = dat$Tv)
dat$ea<-ave(dat$ea, dat$month, dat$year, dat$day, dat$hour)
dat$PAR <- dat$Par_moles_m2_s * 1000000 ### only keep this until push updated to git
dat$PAR<-ave(dat$PAR, dat$month, dat$year, dat$day, dat$hour)
dat$time<-dat$hour
library(dplyr)
dat.check<-dat%>%filter(year==2013)%>%filter(month==8)%>%filter(day==13)%>% ### choose this date due to LAI information
dplyr::select(hour, Tv, relHum, Ca, comp, Vcmax, el, ea, PAR, time)
dat.check<-dat.check[!duplicated(dat.check),]
dat.check<-dat.check[(dat.check$time<24),]
library(plantecophys)
dat.check$VPD<- dat.check$el-dat.check$ea
#dat.check$VPDx<-RHtoVPD(dat.check$relHum, TdegC = dat.check$Tv-273.15)
dat.check$Tair<-dat.check$Tv-273.15
FARAO(Ca=dat.check$Ca, VPD=dat.check$VPD, Tair=dat.check$Tair, PPFD=dat.check$PAR)
lai <- read.csv('~/Documents/git/scaling_farquhar/hf150-01-hem-lai.csv', header=TRUE)
lai<-lai%>%filter(date=="2013-08-13")
lai$lai.sum<-ave(lai$lai.masked, FUN=sum) ## 54.28
###make a data frame for FARAO out put
eco.phys.sol<-as.data.frame(FARAO(Ca=dat.check$Ca, VPD=dat.check$VPD, Tair=dat.check$Tair))
##add a column for the lai sums from harvard forest
eco.phys.sol$HF.lai<-lai$lai.sum
###multiply them together to get the footprint level A
eco.phys.sol$A.footprint<-eco.phys.sol$HF.la*eco.phys.sol$ALEAF
eco.phys.sol$hour<-1:24
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))
library(ggplot2)
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))
colnames(eco.phys.sol)
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point()
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point() + geom_line(aes(x=hour, y=ALEAF))
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="green") + geom_line(aes(x=hour, y=ALEAF), col="green") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
farquhar_solver(input.df = dat.check, stomata = 'open')
far<-as.data.frame(farquhar_solver(input.df = dat.check, stomata = 'open'))
eco.phys.sol$A<-far$AA2
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
geom_line(aes(x=hour, y=A), col="steelblue") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
eco.phys.sol$A<-far$AA1
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
geom_line(aes(x=hour, y=A), col="steelblue") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
eco.phys.sol$A<-far$A.min
ggplot(eco.phys.sol, aes(x=hour, y=ALEAF))+geom_point(col="forestgreen") + geom_line(aes(x=hour, y=ALEAF), col="forestgreen") +
geom_line(aes(x=hour, y=A), col="steelblue") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
axis.ticks.y = element_blank())
#### get the data
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
## housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
## Libraries
library(brms)
head(bb.stan)
#### get the data
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
head(bb.stan)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(nao==-1.583333)
library(dplyr)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(nao==-1.583333)
#### get the data
bb.stan<-read.csv("~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", header=TRUE)
bb.stan<-bb.stan%>%filter(species=="AESHIP")
head(bb.stan)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-5)%>%filter(space<10)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-5)%>%filter(space<5)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-3)%>%filter(space<3)
brm<-brm(fs.count~nao, data=bb.stan, chains=2, family=poisson)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>-1)%>%filter(space<2)
bb.stan<-bb.stan%>%filter(species=="AESHIP")%>%filter(space>0)%>%filter(space<1)
brm<-brm(fs.count~nao, data=bb.stan, chains=2, family=poisson)
save(brm, file="~/Desktop/brm.Rdata")
load("/Users/CatherineChamberlain/Desktop/brm.Rdata")
brm
d<-read.csv("~/Desktop/sandbox/buds_traits.csv")
d<-read.csv("~/Desktop/sandbox/buds_obs.csv")
View(d)
d<-read.csv("~/Documents/git/freezingexperiment/analyses/output/buds_traits.csv")
View(d)
rm(list=ls())
options(stringsAsFactors = FALSE)
fit<-brm(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
d<-read.csv("/n/wolkovich_lab/Lab/Cat/sandbox/buds_traits.csv")
d<-read.csv("~/Documents/git/freezingexperiment/analyses/output/buds_traits.csv")
fit<-brm(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
m<-fit
m.int<-posterior_interval(m)
sum.m<-summary(m)
cri.f<-as.data.frame(sum.m$fixed[,c("Estimate", "l-95% CI", "u-95% CI")])
cri.f<-cri.f[-1,] #removing the intercept
fdf1<-as.data.frame(rbind(as.vector(cri.f[,1]), as.vector(cri.f[,2]), as.vector(cri.f[,3])))
fdf2<-cbind(fdf1, c(0, 0, 0) , c("Estimate", "2.5%", "95%"))
names(fdf2)<-c(rownames(cri.f), "species", "perc")
cri.r<-(ranef(m, summary = TRUE, robust = FALSE,
probs = c(0.025, 0.975)))$species
cri.r2<-cri.r[, ,-1]
cri.r2<-cri.r2[,-2,]
dims<-dim(cri.r2)
twoDimMat <- matrix(cri.r2, prod(dims[1:2]), dims[3])
mat2<-cbind(twoDimMat, c(rep(1:2, length.out=6)), rep(c("Estimate", "2.5%", "95%"), each=2))
df<-as.data.frame(mat2)
names(df)<-c(rownames(cri.f), "species", "perc")
dftot<-rbind(fdf2, df)
dflong<- tidyr::gather(dftot, var, value, nao:sm.elev, factor_key=TRUE)
dflong<- tidyr::gather(dftot, var, value, frost:bud, factor_key=TRUE)
#adding the coef estiamtes to the random effect values
for (i in seq(from=1,to=nrow(dflong), by=9)) {
for (j in seq(from=3, to=8, by=1)) {
dflong$value[i+j]<- as.numeric(dflong$value[i+j]) + as.numeric(dflong$value[i])
}
}
dflong$rndm<-ifelse(dftot$species>0, 2, 1)
dfwide<-tidyr::spread(dflong, perc, value)
dfwide[,4:6] <- as.data.frame(lapply(c(dfwide[,4:6]), as.numeric ))
dfwide$species<-as.factor(dfwide$species)
fit<-lm(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
fit<-lmer(dvr~frost+bud+(1|species)+(frost-1|species)+(bud-1|species), data=d)
load("/Users/CatherineChamberlain/Desktop/sandbox/fit.Rdata")
############################################
## housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
library(ggplot2)
library(rstanarm)
library(dplyr)
library(tidyr)
library(brms)
library(ggstance)
# Setting working directory
setwd("~/Documents/git/regionalrisk/analyses/")
########################
#### get the data
dx<-read.csv("output/fs_matspspace_old.csv", header=TRUE)
bb.stan<-read.csv("output/bb.brm.nointer.csv", header=TRUE)
View(bb.stan)
sort(unique(bb.stan$space))
View(dx)
xx<-read.csv("output/fs_matspspace.csv", header=TRUE)
View(xx)
range(xx$fs.count)
range(dx$fs.count)
range(bb.stan$fs.count)
range(bb.stan$space)
mapWorld <- borders("world", colour="gray72", fill="gray65",ylim=c(30,70),xlim=c(-10,35)) # create a layer of borders
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
library(ggmap)
library(rworldmap)
library(maps)
library(mapdata)
library(marmap)
library(RColorBrewer)
mapWorld <- borders("world", colour="gray72", fill="gray65",ylim=c(30,70),xlim=c(-10,35)) # create a layer of borders
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(-51, 87))
site<- ggplot(bb.stan, aes(x=long, y=lat, col=space), alpha=0.2) +   mapWorld +
coord_cartesian(ylim=c(30,70),xlim=c(-10,35))
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point(alpha=0.2) + geom_jitter()+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
site
############################################
## housekeeping
rm(list=ls())
options(stringsAsFactors = FALSE)
library(ggplot2)
library(rstanarm)
library(dplyr)
library(tidyr)
library(brms)
library(ggstance)
library(ggmap)
library(rworldmap)
library(maps)
library(mapdata)
library(marmap)
library(RColorBrewer)
# Setting working directory
setwd("~/Documents/git/regionalrisk/analyses/")
########################
#### get the data
dx<-read.csv("output/fs_matspspace_old.csv", header=TRUE)
dx<-dx%>%dplyr::select(lat, long, space)
dx<-dx[!duplicated(dx),]
mapWorld <- borders("world", colour="gray72", fill="gray65",ylim=c(30,70),xlim=c(-10,35)) # create a layer of borders
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
xx<-read.csv("output/fs_yearsitespp.csv", header=TRUE)
df<-read.csv("output/mat_fulldata.csv", header=TRUE)
mat<-read.csv("output/fs_bb_sitedata.csv", header=TRUE)
nao<-read.csv("output/nao_year_sp.csv", header=TRUE)
### Clean up dataframes a bit
dx<-full_join(df, dx)
dx<-dx[!duplicated(dx),]
mat<-dplyr::select(mat, species, year, LAT, LON, ALT)
mat<-mat%>%rename(lat=LAT)%>%rename(long=LON)%>%rename(elev=ALT)
mat<-mat[!duplicated(mat),]
bb<-full_join(mat, dx)
#### Get elevation information
bb<-bb%>%rename(sp.temp=pre.bb)
bb$cc<-ifelse(bb$year<=1983&bb$year>=1950, 0, 1)
xx<-dplyr::select(xx, lat, long, species, fs.count)
xx<-xx[!duplicated(xx),]
bb<-full_join(bb, xx)
bb$fs.count<-ave(bb$fs.count, bb$year, bb$lat.long, bb$species, FUN=last)
bb<-bb[!duplicated(bb),]
bb<-na.omit(bb)
nao<-dplyr::select(nao, species, year, m.index)
nao<-nao[!duplicated(nao),]
bb<-full_join(bb, nao)
bb<-bb[!duplicated(bb),]
bb$sm.elev<-bb$elev/100
bb$nao<-bb$m.index*10
columnstokeep.map <- c("species", "nao", "sp.temp", "cc", "space", "sm.elev", "fs.count", "lat", "long")
bb.map<-subset(bb, select=columnstokeep.map)
bb.map<-bb.map[!duplicated(bb.map),]
bb.map<-na.omit(bb.map)
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(-51, 87))
site<- ggplot(bb.map, aes(x=long, y=lat, col=space), alpha=0.2) +   mapWorld +
coord_cartesian(ylim=c(30,70),xlim=c(-10,35))
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point(alpha=0.2) + geom_jitter()+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
site
quartz()
site
columnstokeep.map <- c("space","lat", "long")
bb.map<-subset(bb, select=columnstokeep.map)
bb.map<-bb.map[!duplicated(bb.map),]
write.csv(bb.stan, file="~/Documents/git/regionalrisk/analyses/output/bb.brm.nointer.csv", row.names = FALSE)
bb.map<-na.omit(bb.map)
bb.map.mini<-round(bb.map$space, digits=0)
bb.map.mini<-bb.map
bb.map.mini$space<-round(bb.map$space, digits=0)
bb.map.mini<-bb.map.mini[!duplicated(bb.map.mini$space)]
bb.map.mini<-bb.map.mini[!duplicated(bb.map.mini$space),]
View(bb.map.mini)
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(-51, 87))
site<- ggplot(bb.map, aes(x=long, y=lat, col=space), alpha=0.2) +   mapWorld +
coord_cartesian(ylim=c(30,70),xlim=c(-10,35))
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point(alpha=0.2) + geom_jitter()+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
quartz()
site
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point(alpha=0.2) + geom_jitter(alpha=0.1)+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
quartz()
site
site<- ggplot(bb.map.mini, aes(x=long, y=lat, col=space), alpha=0.2) +   mapWorld +
coord_cartesian(ylim=c(30,70),xlim=c(-10,35))
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point(alpha=0.2) + geom_jitter(alpha=0.1)+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
quartz()
site
bb.map.mini<-bb.map
bb.map.mini$space<-round(bb.map$space, digits=0)
bb.map.mini<-bb.map.mini[!duplicated(bb.map.mini),]
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(-51, 87))
site<- ggplot(bb.map.mini, aes(x=long, y=lat, col=space), alpha=0.2) +   mapWorld +
coord_cartesian(ylim=c(30,70),xlim=c(-10,35))
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point(alpha=0.2) + geom_jitter(alpha=0.1)+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
quartz()
site
site <- site + theme(panel.border = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) + geom_point() + geom_jitter(alpha=0.3)+
sc + labs(color="Space Parameter")+
xlab("Longitude") + ylab("Latitude")
quartz()
site
tiff('test.tiff', units="in", width=5, height=5, res=300)
site
dev.off()
writeRaster(bb.map,"~/Documents/git/regionalrisk/analyses/output/bbspace", bylayer=TRUE,format="GTiff", overwrite=TRUE)
library(raster)
writeRaster(bb.map,"~/Documents/git/regionalrisk/analyses/output/bbspace", bylayer=TRUE,format="GTiff", overwrite=TRUE)
bb.map<-stack(bb.map)
writeRaster(bb.map,"~/Documents/git/regionalrisk/analyses/output/bbspace", bylayer=TRUE,format="GTiff", overwrite=TRUE)
View(bb.map)
columnstokeep.map <- c("space","lat", "long")
bb.map<-subset(bb, select=columnstokeep.map)
bb.map<-bb.map[!duplicated(bb.map),]
bb.map<-na.omit(bb.map)
spg<-bb.map
coordinates(spg)<- ~long+lat
gridded(spg) <- TRUE
View(spg)
View(bb.map)
spg<-bb.map
coordinates(spg)<- ~long+lat
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/bbspace.shp")
rm(list=ls())
library(raster)
setwd("~/Documents/git/regionalrisk/analyses")
d<-read.csv("output/fs_bb_sitedata.csv", header=TRUE)
d<-d[(d$bb>=0),]
a.site<-filter(site, species=="AESHIP")
spg<-a.site
site<-d%>%dplyr::select(LAT, LON, bb.space, species)
site<-site[!duplicated(site),]
a.site<-filter(site, species=="AESHIP")
spg<-a.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/aeship.shp")
ag.site<-filter(site, species=="ALNGLU")
spg<-ag.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/alnglu.shp")
b.site<-filter(site, species=="BETPEN")
spg<-b.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/betpen.shp")
b.site<-filter(site, species=="BETPEN")
spg<-b.site
coordinates(spg)<- ~LON+LAT
View(b.site)
b.site<-na.omit(b.site)
spg<-b.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/betpen.shp")
shapefile(coords, "output/betpen.shp", overwrite=TRUE)
f.site<-filter(site, species=="FAGSYL")
spg<-f.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/fagsyl.shp")
fe.site<-filter(site, species=="FRAEXC")
spg<-fe.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/fraexc.shp")
q.site<-filter(site, species=="QUEROB")
spg<-q.site
coordinates(spg)<- ~LON+LAT
proj4string(spg)<-CRS("+proj=longlat +datum=WGS84")
coords<-spTransform(spg, CRS("+proj=longlat"))
shapefile(coords, "output/querob.shp")
range(site$bb.space)
range(a.site$bb.space)
