install.packages("cowplot")
library(cowplot)
plot_grid(plot1, plot2, labels=c("A", "B"), ncol = 2, nrow = 1)
eur.map <- eur + geom_point(data = europe, aes(Longitude, Latitude, size=False.Springs, color=False.Springs)) +
scale_color_gradient(low="red", high="blue", name="Number of False Springs")  +
guides(size=FALSE)
plot2 <- eur.map
plot_grid(plot1, plot2, labels=c("A", "B"), ncol = 2, nrow = 1)
legend <- get_legend(eur.map)
eur.map <- eur.map + theme(legend.position="none")
grid.arrange(am.map, eur.map, legend, ncol=3, widths=c(2.3, 2.3, 0.8), labels = c("A", "B"))
grid.arrange(am.map, eur.map, legend, ncol=3, widths=c(2.8, 2.8, 0.8), labels = c("A", "B"))
grid.arrange(am.map, eur.map, legend, ncol=3, widths=c(2.8, 2.8, 0.8), labels = c("A", "B", ""))
grid.arrange(am.map, eur.map, legend, ncol=2, nrow = 2,
layout_matrix = rbind(c(1,2), c(3,3)),
widths = c(2.7, 2.7), heights = c(2.5, 0.2))
grid.arrange(am.map, eur.map, legend, ncol=3, widths=c(2.8, 2.8, 0.8))
plot_grid(plot1, plot2, labels=c("A", "B"), ncol = 2, nrow = 1)
plot_grid(plot1, eur.map, labels=c("A", "B"), ncol = 2, nrow = 1)
grid_arrange_shared_legend(plot1, plot2, ncol = 2, labels = c("A", "B"),
widths = c(2.8, 2.8), heights = 2.2)
grid_arrange_shared_legend(plot1, plot2, ncol = 2,
widths = c(2.8, 2.8), heights = 2.2)
rm(list=ls()) # remove everything currently held in the R memory
options(stringsAsFactors=FALSE)
graphics.off()
### The power of Sunny Delight
## Kids who drink sunny d at a young age are more likely to be happier adults
## In our example, if a child drinks 1 bottle of sunny d a week at the age of 5,
# then the amount of dopamine in the brain increases in adulthood.
## The average adult has 45+/-5 ng/ml of dopamine, but kids who drink sunny d
# increase that level to 55 ng/ml
## The first study tests 1000 kids
library(arm)
library(ggplot2)
library(gridExtra)
## What is the effect size in this example?
ES = (55-45)/5 ## ES = 2
SE = 5/(sqrt(10)) ## SE = 0.16
## What about when we triple the effect size?
ES = (75-45)/5 ## ES = 6
SE = 5/(sqrt(10)) ## SE stays the same
## How about when we triple the sample size?
ES = (55-45)/5 ## ES is 2 like before but...
SE = 5/(sqrt(30)) ## SE decreases slightly - SE = 0.09
### Andrew states that " it is generally better to double the effect size [theta] than to double the sample size [n]"
## here we can see that doubling the sample size decreases the standard error at a slower rate than doubling the effect size
# The SE: effect size influences the numerator, sample size influences the denominator
nsunny = 2
rep = 5
ntot = nsunny*rep
sunny = gl(nsunny, rep, length=ntot)
sunnydiff = 10
sunnydiff.sd = 0
suns<-rnorm(ntot, 55, 5)
base <- 45
child <- base + suns-mean(suns)
mm <- model.matrix(~(sunny)^2, data.frame=(sunny))
fake <- vector()
for (i in 1:length(ntot)){
coeff <- c(child[i],
rnorm(1, sunnydiff, sunnydiff.sd)
)
dp <- rnorm(n = length(sunny), mean = mm %*% coeff, sd = 5)
fake <- data.frame(dp=dp, sunny=sunny)
}
hist(fake$dp[sunny==1])
hist(fake$dp[sunny==2])
mean(fake$dp) # 50.5
sd(fake$dp) # 7.24
display(lm(dp~sunny, data=fake))
#lm(formula = dp ~ sunny, data = fake)
#coef.est coef.se
#(Intercept) 45.33     0.23
#sunny2      10.25     0.32
#---
#  n = 1000, k = 2
#residual sd = 5.11, R-Squared = 0.50
### Alright, now let's triple the effect size...
### Sunny D increases the level of dopamine to 75 ng/ml - we'll keep the sample size the same
nsunny.e = 2
rep.e = 5
ntot.e = nsunny.e*rep.e
sunny.e = gl(nsunny.e, rep.e, length=ntot.e)
sunnydiff.e = 30
sunnydiff.sd.e = 0
suns.e<-rnorm(ntot.e, 75, 5)
base.e <- 45
child.e <- base.e + suns.e-mean(suns.e)
mm.e <- model.matrix(~(sunny.e)^2, data.frame=(sunny.e))
fake.e <- vector()
for (i in 1:length(ntot.e)){
coeff.e <- c(child.e[i],
rnorm(1, sunnydiff.e, sunnydiff.sd.e)
)
dp.e <- rnorm(n = length(sunny.e), mean = mm.e %*% coeff.e, sd = 5)
fake.e <- data.frame(dp.e=dp.e, sunny.e=sunny.e)
}
hist(fake.e$dp.e[sunny.e==1])
hist(fake.e$dp.e[sunny.e==2])
mean(fake.e$dp.e) # 61.2
sd(fake.e$dp.e) # 15.9
display(lm(dp.e~sunny.e, data=fake.e))
#lm(formula = dp.e ~ sunny.e, data = fake.e)
#coef.est coef.se
#(Intercept) 46.04     0.22
#sunny.e2    30.24     0.31
#---
#  n = 1000, k = 2
#residual sd = 4.94, R-Squared = 0.90
## And now, we triple the sample size...
nsunny.s = 2
rep.s = 15
ntot.s = nsunny.s*rep.s
sunny.s = gl(nsunny.s, rep.s, length=ntot.s)
sunnydiff.s = 10
sunnydiff.sd.s = 0
suns.s<-rnorm(ntot.s, 55, 5)
base.s <- 45
child.s <- base.s + suns.s-mean(suns.s)
mm.s <- model.matrix(~(sunny.s)^2, data.frame=(sunny.s))
fake.s <- vector()
for (i in 1:length(ntot.s)){
coeff.s <- c(child.s[i],
rnorm(1, sunnydiff.s, sunnydiff.sd.s)
)
dp.s <- rnorm(n = length(sunny.s), mean = mm.s %*% coeff.s, sd = 5)
fake.s <- data.frame(dp.s=dp.s, sunny.s=sunny.s)
}
hist(fake.s$dp.s)
mean(fake.s$dp.s) # 52.8
sd(fake.s$dp.s) # 6.93
display(lm(dp.s~sunny.s, data=fake.s))
#lm(formula = dp.s ~ sunny.s, data = fake.s)
#coef.est coef.se
#(Intercept) 48.01     0.13
#sunny.s2     9.66     0.18
#---
#  n = 3000, k = 2
#residual sd = 4.97, R-Squared = 0.49
#### Let's plot the effects!
fake$sunny<-ifelse(fake$sunny==1, "control", "sunnyD")
base<- qplot(sunny, dp, data = fake, geom="boxplot", color=sunny) +
xlab("Sunny D consumption") + ylab("Dopamine levels (ng/ml)") + ylim(30,85)
fake.e$sunny.e<-ifelse(fake.e$sunny.e==1, "control", "sunnyD")
effect<- qplot(sunny.e, dp.e, data = fake.e, geom="boxplot", color=sunny.e) +
xlab("Sunny D consumption") + ylab("Dopamine levels (ng/ml)") + ylim(30,85)
fake.s$sunny.s<-ifelse(fake.s$sunny.s==1, "control", "sunnyD")
sample<- qplot(sunny.s, dp.s, data = fake.s, geom="boxplot", color=sunny.s) +
xlab("Sunny D consumption") + ylab("Dopamine levels (ng/ml)") + ylim(30,85)
grid.arrange(base, effect, sample, ncol=3, nrow=1)
fake$child<- as.numeric(sample(1000))
fake.e$child.e<-as.numeric(sample(1000))
fake.s$child.s<-as.numeric(sample(3000))
bg<-ggplot(fake, aes(x=child, y=dp)) + geom_point(aes(color=sunny)) + geom_smooth(method="lm")
#bh<-hist(fake$dp)
#grid.arrange(bg,bh, ncol=1, nrow=2)
eg<-ggplot(fake.e, aes(x=child.e, y=dp.e)) + geom_point(aes(color=sunny.e)) + geom_smooth(method="lm")
#eh<-hist(fake.e$dp.e)
#grid.arrange(eg,eh, ncol=1, nrow=2)
sg<-ggplot(fake.s, aes(x=child.s, y=dp.s)) + geom_point(aes(color=sunny.s)) + geom_smooth(method="lm")
#sh<-hist(fake.s$dp.s)
#grid.arrange(sg,sh, ncol=1, nrow=2)
grid.arrange(bg,eg,sg, ncol=3, nrow=1)
fake$child<- as.numeric(sample(10))
fake.e$child.e<-as.numeric(sample(10))
fake.s$child.s<-as.numeric(sample(30))
bg<-ggplot(fake, aes(x=child, y=dp)) + geom_point(aes(color=sunny)) + geom_smooth(method="lm")
eg<-ggplot(fake.e, aes(x=child.e, y=dp.e)) + geom_point(aes(color=sunny.e)) + geom_smooth(method="lm")
sg<-ggplot(fake.s, aes(x=child.s, y=dp.s)) + geom_point(aes(color=sunny.s)) + geom_smooth(method="lm")
grid.arrange(bg,eg,sg, ncol=3, nrow=1)
library(arm)
library(ggplot2)
library(gridExtra)
## What is the effect size in this example?
ES = (55-45)/5 ## ES = 2
SE = 5/(sqrt(100)) ## SE = 0.16
## What about when we triple the effect size?
ES = (75-45)/5 ## ES = 6
SE = 5/(sqrt(100)) ## SE stays the same
## How about when we triple the sample size?
ES = (55-45)/5 ## ES is 2 like before but...
SE = 5/(sqrt(300)) ## SE decreases slightly - SE = 0.09
### Andrew states that " it is generally better to double the effect size [theta] than to double the sample size [n]"
## here we can see that doubling the sample size decreases the standard error at a slower rate than doubling the effect size
# The SE: effect size influences the numerator, sample size influences the denominator
nsunny = 2
rep = 50
ntot = nsunny*rep
sunny = gl(nsunny, rep, length=ntot)
sunnydiff = 10
sunnydiff.sd = 0
suns<-rnorm(ntot, 55, 5)
base <- 45
child <- base + suns-mean(suns)
mm <- model.matrix(~(sunny)^2, data.frame=(sunny))
fake <- vector()
for (i in 1:length(ntot)){
coeff <- c(child[i],
rnorm(1, sunnydiff, sunnydiff.sd)
)
dp <- rnorm(n = length(sunny), mean = mm %*% coeff, sd = 5)
fake <- data.frame(dp=dp, sunny=sunny)
}
hist(fake$dp[sunny==1])
hist(fake$dp[sunny==2])
mean(fake$dp) # 50.5
sd(fake$dp) # 7.24
display(lm(dp~sunny, data=fake))
#lm(formula = dp ~ sunny, data = fake)
#coef.est coef.se
#(Intercept) 45.33     0.23
#sunny2      10.25     0.32
#---
#  n = 1000, k = 2
#residual sd = 5.11, R-Squared = 0.50
### Alright, now let's triple the effect size...
### Sunny D increases the level of dopamine to 75 ng/ml - we'll keep the sample size the same
nsunny.e = 2
rep.e = 50
ntot.e = nsunny.e*rep.e
sunny.e = gl(nsunny.e, rep.e, length=ntot.e)
sunnydiff.e = 30
sunnydiff.sd.e = 0
suns.e<-rnorm(ntot.e, 75, 5)
base.e <- 45
child.e <- base.e + suns.e-mean(suns.e)
mm.e <- model.matrix(~(sunny.e)^2, data.frame=(sunny.e))
fake.e <- vector()
for (i in 1:length(ntot.e)){
coeff.e <- c(child.e[i],
rnorm(1, sunnydiff.e, sunnydiff.sd.e)
)
dp.e <- rnorm(n = length(sunny.e), mean = mm.e %*% coeff.e, sd = 5)
fake.e <- data.frame(dp.e=dp.e, sunny.e=sunny.e)
}
hist(fake.e$dp.e[sunny.e==1])
hist(fake.e$dp.e[sunny.e==2])
mean(fake.e$dp.e) # 61.2
sd(fake.e$dp.e) # 15.9
display(lm(dp.e~sunny.e, data=fake.e))
#lm(formula = dp.e ~ sunny.e, data = fake.e)
#coef.est coef.se
#(Intercept) 46.04     0.22
#sunny.e2    30.24     0.31
#---
#  n = 1000, k = 2
#residual sd = 4.94, R-Squared = 0.90
## And now, we triple the sample size...
nsunny.s = 2
rep.s = 150
ntot.s = nsunny.s*rep.s
sunny.s = gl(nsunny.s, rep.s, length=ntot.s)
sunnydiff.s = 10
sunnydiff.sd.s = 0
suns.s<-rnorm(ntot.s, 55, 5)
base.s <- 45
child.s <- base.s + suns.s-mean(suns.s)
mm.s <- model.matrix(~(sunny.s)^2, data.frame=(sunny.s))
fake.s <- vector()
for (i in 1:length(ntot.s)){
coeff.s <- c(child.s[i],
rnorm(1, sunnydiff.s, sunnydiff.sd.s)
)
dp.s <- rnorm(n = length(sunny.s), mean = mm.s %*% coeff.s, sd = 5)
fake.s <- data.frame(dp.s=dp.s, sunny.s=sunny.s)
}
hist(fake.s$dp.s)
mean(fake.s$dp.s) # 52.8
sd(fake.s$dp.s) # 6.93
display(lm(dp.s~sunny.s, data=fake.s))
#lm(formula = dp.s ~ sunny.s, data = fake.s)
#coef.est coef.se
#(Intercept) 48.01     0.13
#sunny.s2     9.66     0.18
#---
#  n = 3000, k = 2
#residual sd = 4.97, R-Squared = 0.49
#### Let's plot the effects!
fake$sunny<-ifelse(fake$sunny==1, "control", "sunnyD")
base<- qplot(sunny, dp, data = fake, geom="boxplot", color=sunny) +
xlab("Sunny D consumption") + ylab("Dopamine levels (ng/ml)") + ylim(30,85)
fake.e$sunny.e<-ifelse(fake.e$sunny.e==1, "control", "sunnyD")
effect<- qplot(sunny.e, dp.e, data = fake.e, geom="boxplot", color=sunny.e) +
xlab("Sunny D consumption") + ylab("Dopamine levels (ng/ml)") + ylim(30,85)
fake.s$sunny.s<-ifelse(fake.s$sunny.s==1, "control", "sunnyD")
sample<- qplot(sunny.s, dp.s, data = fake.s, geom="boxplot", color=sunny.s) +
xlab("Sunny D consumption") + ylab("Dopamine levels (ng/ml)") + ylim(30,85)
grid.arrange(base, effect, sample, ncol=3, nrow=1)
fake$child<- as.numeric(sample(100))
fake.e$child.e<-as.numeric(sample(100))
fake.s$child.s<-as.numeric(sample(300))
bg<-ggplot(fake, aes(x=child, y=dp)) + geom_point(aes(color=sunny)) + geom_smooth(method="lm")
eg<-ggplot(fake.e, aes(x=child.e, y=dp.e)) + geom_point(aes(color=sunny.e)) + geom_smooth(method="lm")
sg<-ggplot(fake.s, aes(x=child.s, y=dp.s)) + geom_point(aes(color=sunny.s)) + geom_smooth(method="lm")
grid.arrange(bg,eg,sg, ncol=3, nrow=1)
display(mod.base);display(mod.e);display(mod.s)
mod.base<-lm(dp~sunny, data=fake)
mod.e<-lm(dp.e~sunny.e, data=fake.e)
mod.s<-lm(dp.s~sunny.s, data=fake.s)
display(mod.base);display(mod.e);display(mod.s)
rm(list=ls()) # remove everything currently held in the R memory
options(stringsAsFactors=FALSE)
graphics.off()
# Load libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(ncdf4)
library(Interpol.T)
library(chillR)
library(raster)
library(reshape2)
library(data.table)
library(arm)
setwd("~/Documents/git/regionalrisk/analyses")
d<-read.csv("output/bbch_region.csv", header=TRUE)
### Let's just start with the PEP data and do some cleaning
df<-d%>%
filter(BBCH==11)%>%
filter(YEAR>=1950)%>%
dplyr::select(YEAR, DAY, BBCH, PEP_ID, LAT, LON)%>%
rename(year=YEAR)%>%
rename(bb=DAY)%>%
rename(lat=LAT)%>%
rename(long=LON)
table(df$PEP_ID)
df<-df %>% group_by(PEP_ID, year) %>% filter(row_number(BBCH) == 1)
table(df$PEP_ID)
table(df$BBCH)
table(df$BBCH, df$year)
table(df$PEP_ID, df$year)
rm(list=ls()) # remove everything currently held in the R memory
options(stringsAsFactors=FALSE)
graphics.off()
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(ncdf4)
library(Interpol.T)
library(chillR)
library(raster)
library(reshape2)
library(data.table)
library(arm)
setwd("~/Documents/git/regionalrisk/analyses")
d<-read.csv("output/bbch_region_alnus.csv", header=TRUE)
df<-d%>%
filter(BBCH==11)%>%
filter(YEAR>=1950)%>%
dplyr::select(YEAR, DAY, BBCH, PEP_ID, LAT, LON)%>%
rename(year=YEAR)%>%
rename(bb=DAY)%>%
rename(lat=LAT)%>%
rename(long=LON)
df$lo<-df$bb+12
df<- df[order(df$PEP_ID, df$year), ]
df$pep.year<-paste(df$year, df$PEP_ID)
days.btw <- Map(seq, df$bb, df$lo, by = 1)
dxx <- data.frame(PEP_ID=df$PEP_ID, lat=df$lat, long=df$long,
pep.year = rep.int(df$pep.year, vapply(days.btw, length, 1L)),
doy = do.call(c, days.btw))
dxx$year<-substr(df$pep.year, 1, 4)
dxx<-dplyr::select(dxx, -pep.year)
x<-paste(dxx$year, dxx$doy)
dxx$date<-as.Date(strptime(x, format="%Y %j"))
dxx$Date<- as.character(dxx$date)
dxx<-arrange(dxx, PEP_ID, year)
dxx<-arrange(dxx, PEP_ID, Date)
head(dxx)
df<-d%>%
filter(BBCH==11)%>%
filter(YEAR>=1950)%>%
dplyr::select(YEAR, DAY, BBCH, PEP_ID, LAT, LON)%>%
rename(year=YEAR)%>%
rename(bb=DAY)%>%
rename(lat=LAT)%>%
rename(long=LON)
df$lo<-df$bb+12
head(df)
df<-arrange(df, PEP_ID, year)
head(df)
df$pep.year<-paste(df$year, df$PEP_ID)
days.btw <- Map(seq, df$bb, df$lo, by = 1)
dxx <- data.frame(PEP_ID=df$PEP_ID, lat=df$lat, long=df$long,
pep.year = rep.int(df$pep.year, vapply(days.btw, length, 1L)),
doy = do.call(c, days.btw))
head(dxx)
dxx$year<-substr(df$pep.year, 1, 4)
dxx<-dplyr::select(dxx, -pep.year)
x<-paste(dxx$year, dxx$doy)
dxx$date<-as.Date(strptime(x, format="%Y %j"))
dxx$Date<- as.character(dxx$date)
head(dxx)
dxx<-arrange(dxx, PEP_ID, year)
head(dxx)
df<-d%>%
filter(BBCH==11)%>%
filter(YEAR>=1950)%>%
dplyr::select(YEAR, DAY, BBCH, PEP_ID, LAT, LON)%>%
rename(year=YEAR)%>%
rename(bb=DAY)%>%
rename(lat=LAT)%>%
rename(long=LON)
df$lo<-df$bb+12
## Hmm... can we sequence from budburst to leafout to find the number of freezes between?
df<- df[order(df$PEP_ID, df$year), ]
df<-arrange(df, PEP_ID, year)
df$pep.year<-paste(df$year, df$PEP_ID)
days.btw <- Map(seq, df$bb, df$lo, by = 1)
dxx <- data.frame(PEP_ID=df$PEP_ID, lat=df$lat, long=df$long,
pep.year = rep.int(df$pep.year, vapply(days.btw, length, 1L)),
doy = do.call(c, days.btw))
head(dxx)
dxx$year<-substr(df$pep.year, 1, 4)
head(dxx)
dxx$year<-substr(dxx$pep.year, 1, 4)
head(dxx)
dxx<-dplyr::select(dxx, -pep.year)
x<-paste(dxx$year, dxx$doy)
dxx$date<-as.Date(strptime(x, format="%Y %j"))
dxx$Date<- as.character(dxx$date)
head(dxx)
r<-brick("~/Desktop/tn_0.25deg_reg_v16.0.nc", varname="tn", sep="")
bb<-dxx
bb$lat.long<-paste(bb$lat, bb$long, sep=",")
bb<-bb[!duplicated(bb$lat.long),]
lats <- bb$lat
lons <- bb$long
coords <- data.frame(x=lons,y=lats)
points <- SpatialPoints(coords, proj4string = r@crs)
values <- extract(r,points)
dclim <- cbind.data.frame(coordinates(points),values)
dx<-melt(dclim, id.vars=c("x","y"))
dx<-dx%>%
rename(long=x)%>%
rename(lat=y)%>%
rename(date=variable)%>%
rename(Tmin=value)
dx$date<-substr(dx$date, 2,11)
dx$Date<-gsub("[.]","-", dx$date)
dxx<-dplyr::select(dxx, -date)
dx<-dplyr::select(dx, -date)
alnglu<-inner_join(dx, dxx, by=c("Date", "lat", "long"))
any.nas<-alnglu[is.na(alnglu$Tmin),]
write.csv(alnglu, file="~/Documents/git/regionalrisk/analyses/output/alnglu_data.csv", row.names=FALSE)
rm(list=ls()) # remove everything currently held in the R memory
options(stringsAsFactors=FALSE)
graphics.off()
# Load libraries
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(ncdf4)
library(Interpol.T)
library(chillR)
library(raster)
library(reshape2)
library(data.table)
library(arm)
setwd("~/Documents/git/regionalrisk/analyses")
d<-read.csv("output/bbch_region_fraxinus.csv", header=TRUE)
### Let's just start with the PEP data and do some cleaning
df<-d%>%
filter(BBCH==11)%>%
filter(YEAR>=1950)%>%
dplyr::select(YEAR, DAY, BBCH, PEP_ID, LAT, LON)%>%
rename(year=YEAR)%>%
rename(bb=DAY)%>%
rename(lat=LAT)%>%
rename(long=LON)
df$lo<-df$bb+12
## Hmm... can we sequence from budburst to leafout to find the number of freezes between?
df<- df[order(df$PEP_ID, df$year), ]
df<-arrange(df, PEP_ID, year)
df$pep.year<-paste(df$year, df$PEP_ID)
days.btw <- Map(seq, df$bb, df$lo, by = 1)
dxx <- data.frame(PEP_ID=df$PEP_ID, lat=df$lat, long=df$long,
pep.year = rep.int(df$pep.year, vapply(days.btw, length, 1L)),
doy = do.call(c, days.btw))
dxx$year<-substr(dxx$pep.year, 1, 4)
dxx<-dplyr::select(dxx, -pep.year)
x<-paste(dxx$year, dxx$doy)
dxx$date<-as.Date(strptime(x, format="%Y %j"))
dxx$Date<- as.character(dxx$date)
## Climate Data time...
r<-brick("~/Desktop/tn_0.25deg_reg_v16.0.nc", varname="tn", sep="")
bb<-dxx
bb$lat.long<-paste(bb$lat, bb$long, sep=",")
bb<-bb[!duplicated(bb$lat.long),]
lats <- bb$lat
lons <- bb$long
coords <- data.frame(x=lons,y=lats)
points <- SpatialPoints(coords, proj4string = r@crs)
values <- extract(r,points)
dclim <- cbind.data.frame(coordinates(points),values)
dx<-melt(dclim, id.vars=c("x","y"))
dx<-dx%>%
rename(long=x)%>%
rename(lat=y)%>%
rename(date=variable)%>%
rename(Tmin=value)
dx$date<-substr(dx$date, 2,11)
dx$Date<-gsub("[.]","-", dx$date)
dxx<-dplyr::select(dxx, -date)
dx<-dplyr::select(dx, -date)
fraexc<-inner_join(dx, dxx, by=c("Date", "lat", "long"))
any.nas<-fraexc[is.na(fraexc$Tmin),]
write.csv(fraexc, file="~/Documents/git/regionalrisk/analyses/output/fraexc_data.csv", row.names=FALSE)
